#Using properties sys: means system property, const: means java constant
#user.file = ${sys:user.home}/settings.xml
#action.key = ${const:java.awt.event.KeyEvent.VK_CANCEL}
# see http://commons.apache.org/configuration/userguide/howto_basicfeatures.html#Basic_features_and_AbstractConfiguration

writer.basedir=/data/streams-collector/logs
java.library.path=/opt/streams-collector/lib/native/Linux-amd64-64

writer.logname.keys=logType
#files that reach near to this size in megabytes will be rolled
writer.logsize=127
#A file with creation time (milliseconds) larger than this value will be rolled
writer.logrotate.time=3600000
#An open file that has not received data from the agents longer than this time (milliseconds) will be rolled.
writer.logrotate.inactivetime=60000
#check for log rotation every second (time in milliseconds). 
#This operation should be run frequently to close files not needed
writer.logrotate.check.period=1000


#note if using LZO use LzopCodec, the LzoCodec class will return .lzo_deflate as the default lzo extension, LzopCodec returns .lzo
writer.compressions.codec=com.hadoop.compression.lzo.LzopCodec
writer.compress.output=true

collector.mon.port=8080
collector.port=8210
coordination.host=localhost

#the application will throw an IO exception when trying to open a new file if this limit is reached.
openfile.limit=2000

coordination.lock.port=5420
coordination.unlock.port=5430

#### COLLECTOR THREAD POOLS
## DEFAULT IS CACHED types accepted FIXED, MEMORY
collector.worker.thread.pool=CACHED
## if type is FIXED
#collector.worker.thread.count=100
#collector.workerboss.thread.count=2
## if type MEMORY
#default one meg
#collector.worker.channel.memorysize=1048576
#default one gig
#collector.worker.total.memorysize=1073741824